# 线性回归 #
[线性回归](https://www.jianshu.com/p/a360f139b076)
```Python
import numpy as np
import random
import time


# 线性回归模型预测
def inference(k,b,x):
    pred_y = k * x + b
    return pred_y

# 损失函数定义
def eval_loss(k, b, x_list, t_y_list):
	avg_loss = 0
	for i in range(len(x_list)):
		avg_loss += 0.5*pow((k * x_list[i] + b - t_y_list[i]),2)

# 单一样本带来的梯度
def gradient(pred_y, t_y, x)
	diff = pre_y - t_y
	dk = diff
	db = diff
	return dk, db

# 全部样本(batchsize)为k,b带来的更新
def cal_step_gradient(batch_x_list, batch_t_y_list, w, b ,lr):
	avg_dw, avg_db = 0, 0
    batch_size = len(batch_x_list)
	for i in range(batch_x_list):
		 pred_y = inference(k, b, batch_x_list[i])
# 生成数据
def gen_sample_data():
    w = random.randint(0,10) + random.random()
    b = random.randint(0, 5) + random.random()
    
    num_sample = 100
    x_list = []
    y_list = []
    print(k,b)
    for i in range(num_sample):
        x = random.randint(0,100) * random.random()
        y = k * x + b + random.random() * random.randint(-1, 100)
        
        x_list.append(x)
        y_list.append(y)
        
    return x_list, y_list

# 训练数据
def train(x_list, gt_y_list, batch_size, lr, max_iter):
    k = 0
    b = 0
    num_samples = len(x_list)
    for i in range(max_iter):
        batch_idxs = np.random.choice(len(x_list), batch_size) #随机抽取batch_size个样本的索引值
        batch_x = [x_list[j] for j in batch_idxs]
        batch_y = [gt_y_list[j] for j in batch_idxs]
        k, b = cal_step_gradient(batch_x, batch_y, k, b, lr)
        print('k:{0},b:{1}'.format(k,b))
        print('loss is {}'.format(eval_loss(k,b,x_list,gt_y_list)))
        time.sleep(0.1)
        
    return k,b

train(x_list, y_list, 100, 0.001, 100)
```